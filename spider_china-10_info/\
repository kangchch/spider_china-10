
# -*- coding: utf-8 -*-
##
# @file spider_china.py
# @brief www.china-10.com
# @author kangchch
# @version 1.0
# @date 2017-11-21


from scrapy.http import Request
import xml.etree.ElementTree
from scrapy.selector import Selector

import scrapy
import re
from pymongo import MongoClient
from copy import copy
import traceback
import pymongo
from scrapy import log
from spider_china_10.items import SpiderChina10Item
import time
import datetime
import sys
import logging
import random
import binascii
from scrapy.conf import settings
import json

reload(sys)
sys.setdefaultencoding('utf-8')


class SpiderChina10Spider(scrapy.Spider):
    name = "spider_china"

    def __init__(self, settings, *args, **kwargs):
        super(SpiderChina10Spider, self).__init__(*args, **kwargs)
        self.settings = settings
        mongo_info = settings.get('MONGO_INFO', {})

        try:
            self.mongo_db = pymongo.MongoClient(mongo_info['host'], mongo_info['port']).china_info
        except Exception, e:
            self.log('connect mongo 192.168.60.65:10010 failed! (%s)' % (str(e)), level=log.CRITICAL)
            raise scrapy.exceptions.CloseSpider('initialization mongo error (%s)' % (str(e)))

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.settings)

    def start_requests(self):

        try:
            start_url = 'http://www.china-10.com'
            # self.log('spider new url=%s' % (company_url), level=log.INFO)
            yield scrapy.Request(url = start_url, callback = self.parse_first_page, dont_filter = True)
        except:
            self.log('start_request error! (%s)' % (str(traceback.format_exc())), level=log.INFO)

    # 解析一级页面
    def parse_first_page(self, response):
        sel = Selector(response)

        ret_item = SpiderChina10Item()
        ret_item['update_item'] = {}
        i = ret_item['update_item']

        if response.status != 200 or len(response.body) <= 0:
            self.log('fetch failed ! status = %d, ' % (response.status), level = log.WARNING)

        ## first_classify 一级分类
        first_classifys = re.findall("(?<=/> ).*?(?=</a>)", response.body, re.S) ##一级分类名称

        second_xpaths = sel.xpath("//*[@id='conmenu']/li[1]/div/ul/li/a") ## 家居建材五金下的分类(名称、url)
        for second_xpath in second_xpaths:
            # for first_classify in first_classifys:
            i['first_name'] = first_classifys[0].strip() ## 一级分类名称
            i['second_name'] = second_xpath.xpath("text()")[0].extract() ## 二级名称
            i['second_url'] = second_xpath.xpath("@href")[0].extract()  ## 二级url
            yield ret_item

            # self.log(' . first_name:%s, second_name:%s ' % (i['first_name'], i['second_name']), level=log.INFO)
            meta = {'dont_redirect': True, 'item': ret_item, 'dont_retry': True}
            yield scrapy.Request(url = i['second_url'], meta = meta, callback = self.parse_second_page, dont_filter = True)

    # 解析二级页面
    def parse_second_page(self, response):
        sel = Selector(response)

        ret_item = response.meta['item']
        i = ret_item['update_item']

        three_xpaths = sel.xpath("//a[@class='red dhidden']") ## 三级分类（名称、url）
        for three_xpath in three_xpaths:
            i['three_name'] = three_xpath.xpath("text()")[0].extract()
            i['three_url'] = three_xpath.xpath("@href")[0].extract()

            self.log(' . first_name:%s, second_name:%s, three_name:%s ' % (i['first_name'], i['second_name'], i['three_name']), level=log.INFO)
            meta = {'dont_redirect': True, 'item': ret_item, 'dont_retry': True}
            # yield scrapy.Request(url = i['three_url'], meta = meta, callback = self.parse_three_page, dont_filter = True)

    # 解析三级页面
    def parse_three_page(self, response):
        sel = Selector(response)

        ret_item = response.meta['item']
        i = ret_item['update_item']

        meta = {'dont_redirect': True, 'item': ret_item, 'dont_retry': True}
        yield scrapy.Request(url = start_url, meta = meta, callback = self.parse_second_page, dont_filter = True)
